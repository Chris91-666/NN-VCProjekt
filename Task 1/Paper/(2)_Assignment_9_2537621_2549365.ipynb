{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GGdDzNp-gEg"
   },
   "source": [
    "# Assignment 9 (Exercise 9.3)\n",
    "\n",
    "## Simon Laurent Lebailly, 2549365, s9sileba@teams.uni-saarland.de\n",
    "## Christian Mathieu Schmidt, 2537621, s9cmscmi@teams.uni-saarland.de\n",
    "\n",
    "\n",
    "\n",
    "### In the lecture you have been introduced to VGG16. For this problem your task is to implement a VGG like CNN architecture for classification on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "BQs8RyYL9nUc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ1AZfUD_HZ9"
   },
   "source": [
    "### 1. Load the dataset (0.5 point)\n",
    "To load the dataset, you can use the inbuilt dataloader for CIFAR10 provided in the torchvision package. Load both test set and trainset separately. Define the transformations you might need to load the data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOSZHQRT9nUr",
    "outputId": "d162efa6-ebb5-4bcc-93f5-3bd6f8779048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available!\n",
      "Files already downloaded and verified\n",
      "Quantity training data: 50000\n",
      "Files already downloaded and verified\n",
      "Quantity test data: 10000\n"
     ]
    }
   ],
   "source": [
    "#Import datasets from torchvision to import the CIFAR10 dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "#Check if CUDA is available, if not use the CPU. \n",
    "#We have worked on a PC (CUDA available) and a Macbook (CUDA not available), so we have used both.\n",
    "#Therefore we have implemented a case distinction, and we have modified the code where necessary.\n",
    "train_on_GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if train_on_GPU else 'cpu')\n",
    "\n",
    "if train_on_GPU:\n",
    "    print('CUDA available!')\n",
    "else:\n",
    "    print('CUDA not available!')\n",
    "\n",
    "\n",
    "#Hyperparameters:\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10('data', train=True, download=True, transform=transform)\n",
    "quantity_train = len(trainset)\n",
    "print('Quantity training data: '+ str(quantity_train))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "testset = datasets.CIFAR10('data', train=False, download=True, transform=transform)\n",
    "quantity_test = len(testset)\n",
    "print('Quantity test data: '+ str(quantity_test))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGpxcaoc_te7"
   },
   "source": [
    "### Create the model architecture (1.0 point)\n",
    "Implement the class below such that the final architecture follows the same pattern of layers as VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "BeCGIwT99nUs"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #Pooling filter with size 2x2, to halve the size of the image\n",
    "        self.pool = nn.MaxPool2d(2,2)        \n",
    "        \n",
    "        #First VGG layer with two times Convolution and duplicating of the features\n",
    "        self.conv11 = nn.Conv2d(3,32,3)\n",
    "        self.conv12 = nn.Conv2d(32,64,14,stride=2)\n",
    "        \n",
    "        #Second VGG layer with three times Convolution and duplicating of the features\n",
    "        self.conv21 = nn.Conv2d(16,32,3)\n",
    "        self.conv22 = nn.Conv2d(32,64,5,stride=2)\n",
    "        self.conv23 = nn.Conv2d(64,32,3)\n",
    "        \n",
    "        #Third VGG layer with fully connected layers\n",
    "        self.fc1 = nn.Linear(32*4*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #First VGG layer with two times Convolution + ReLu and duplicating of the features\n",
    "        #Pooling filter with size 2x2, to halve the size of the image, after the second convolution\n",
    "        out = F.relu(self.conv11(x))\n",
    "        print(out.size())\n",
    "        out = F.relu(self.conv12(out))\n",
    "        print(out.size())\n",
    "        \n",
    "        #Second VGG layer with three times Convolution + ReLu and duplicating of the features\n",
    "        #Pooling filter with size 2x2, to halve the size of the image, after the third convolution\n",
    "        out = F.relu(self.conv21(out))\n",
    "        out = F.relu(self.conv22(out))\n",
    "        out = self.pool(F.relu(self.conv23(out)))\n",
    "        \n",
    "        #Third VGG layer with fully connected layers\n",
    "        out = out.view(-1, 32*4*4)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        #We don't use SOFTMAX in the forward process, because we use CrossEntropyLoss() as criterion\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbNC9ujQAJY6"
   },
   "source": [
    "### Loss function and optimizer (0.5 point)\n",
    "Define the loss function and optimizer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "s5bALqHB9nUt"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWmCHNJSAUe1"
   },
   "source": [
    "### Train the model (1.0 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pF7eNJ389nUt",
    "outputId": "6d295863-682c-4fa4-92b3-9040956f01a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 30, 30])\n",
      "torch.Size([4, 64, 9, 9])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 16, 3, 3], expected input[4, 64, 9, 9] to have 16 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1fb592cc176b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#Compute the forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-301d5a5dfe24>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#Second VGG layer with three times Convolution + ReLu and duplicating of the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#Pooling filter with size 2x2, to halve the size of the image, after the third convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv23\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 16, 3, 3], expected input[4, 64, 9, 9] to have 16 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "#Import matplotlib to show the course of the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Two lists  only used for the plot and the statistics\n",
    "lossdata = []\n",
    "mean_lossdata = []\n",
    "\n",
    "#We have used 16 epochs, because we had the best results (>= 70% accuracy) in dependence of the time the CNN needs to train (ca. 20min).\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #\"Transport\" the data to CUDA if available, and otherwise to the CPU\n",
    "        image = data[0].to(device)\n",
    "        label = data[1].to(device)\n",
    "        \n",
    "        #Compute the forward propagation\n",
    "        output = net(image)\n",
    "        \n",
    "        #Compute the loss\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        #Reset gradient and execute backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print statistics\n",
    "        if ((i+1) % 10000 == 0) or ((i+1) % (50000/batch_size) == 0):\n",
    "            lossdata.append(loss.item())\n",
    "            mean_loss = sum(lossdata)/len(lossdata)\n",
    "            mean_lossdata.append(mean_loss)\n",
    "            \n",
    "            print(\"Progress: \" + str(round(((((i+1)+epoch*12000)*100)/((50000/batch_size)*epochs)), 2)) + \"%, Epoch: \" + str(epoch+1) \n",
    "                  + \", Datasets: \" + str((i+1)+epoch*12000) + \", Loss: \" + str(loss.item()) + \", Mean Loss: \" + str(mean_loss))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "#Plot the loss function\n",
    "plt.title(\"Loss during the training of the CNN\")\n",
    "plt.plot(lossdata,'-b',label='Loss')\n",
    "plt.plot(mean_lossdata,'-r',label='Mean Loss')\n",
    "plt.xlabel('Number of datasets divided by 1000')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLlqZjZiA5nf"
   },
   "source": [
    "Code below generates the class wise accuracy of the model. You can use the results from the code below to decide the values of hyperparametrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ughmKpRj9nUu",
    "outputId": "fffdb66d-2876-4689-803e-e0412d7950a3"
   },
   "outputs": [],
   "source": [
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        #We have set .cuda() to .to(device) because of the different systems we have used.\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "total_class = 0\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    total_class += class_correct[i]\n",
    "\n",
    "\n",
    "#We have added the value of the overall accuracy.\n",
    "total = 100 * total_class / (len(testloader)*batch_size)\n",
    "\n",
    "print(\" \")\n",
    "print('Accuracy total: ' + str(total) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ei4AsQ6_-TXW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
